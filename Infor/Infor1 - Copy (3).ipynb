{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\")\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\pc\\Desktop\\Data Science Folder\\Infor\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\pc\\Desktop\\Data Science Folder\\Infor\\test.csv')\n",
    "submission = pd.read_csv(r'C:\\Users\\pc\\Desktop\\Data Science Folder\\Infor\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n******* train ********\\n', 'shape: ', train.shape, '\\n', train.columns, '\\n',\\\n",
    "      train.describe(), '\\n', train.isnull().sum() )\n",
    "\n",
    "print('\\n******* test ********\\n', 'shape: ', test.shape, '\\n', test.columns, '\\n', \\\n",
    "      test.describe(), '\\n', test.isnull().sum() )\n",
    "\n",
    "print('\\n******* submission ********\\n', 'shape: ', submission.shape, '\\n', submission.columns, '\\n', \\\n",
    "      submission.describe(), '\\n', submission.isnull().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n",
    " #              'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n",
    "  #             'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\n",
    "\n",
    "\n",
    "rf_features = X_train.columns\n",
    "\n",
    "\n",
    "rf_train = X_train[rf_features]\n",
    "rf_val = X_validation[rf_features]\n",
    "rf_test = X_test[rf_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\n",
    "rf_model.fit(rf_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_train_pred = rf_model.predict(rf_train)\n",
    "rf_val_pred = rf_model.predict(rf_val)\n",
    "rf_test_pred = rf_model.predict(rf_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(rf_train_pred, Y_train))\n",
    "print('rmse of train : ' , rmse)\n",
    "               \n",
    "rmse = np.sqrt(mean_squared_error(rf_val_pred, Y_validation))\n",
    "print('rmse of validation : ' , rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\n",
    "\n",
    "lr_features = X_train.columns\n",
    "\n",
    "\n",
    "lr_train = X_train[lr_features]\n",
    "lr_val = X_validation[lr_features]\n",
    "lr_test = X_test[lr_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_scaler = MinMaxScaler()\n",
    "lr_scaler.fit(lr_train)\n",
    "lr_train = lr_scaler.transform(lr_train)\n",
    "lr_val = lr_scaler.transform(lr_val)\n",
    "lr_test = lr_scaler.transform(lr_test)\n",
    "lr_model = LinearRegression(n_jobs=-1)\n",
    "lr_model.fit(lr_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_pred = lr_model.predict(lr_train)\n",
    "lr_val_pred = lr_model.predict(lr_val)\n",
    "lr_test_pred = lr_model.predict(lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(lr_train_pred, Y_train))\n",
    "print('rmse of train : ' , rmse)\n",
    "               \n",
    "rmse = np.sqrt(mean_squared_error(lr_val_pred, Y_validation))\n",
    "print('rmse of validation : ' , rmse)\n",
    "\n",
    "print( lr_model.score(lr_train, Y_train) )\n",
    "print( lr_model.score(lr_val, Y_validation) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model=LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=32,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.04,\n",
    "        reg_lambda=0.073,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "lightgbm_train_pred = lgbm_model.predict(X_train)\n",
    "lightgbm_val_pred = lgbm_model.predict(X_validation)\n",
    "lightgbm_test_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_train, lightgbm_train_pred ))\n",
    "print('rmse of train : ' , rmse)\n",
    "               \n",
    "rmse = np.sqrt(mean_squared_error(Y_validation, lightgbm_val_pred))\n",
    "print('rmse of validation : ' , rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only part of features on XGBoost.\n",
    "#xgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n",
    " #               'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n",
    "  #              'shop_item_mean', 'item_trend', 'mean_item_cnt']\n",
    "\n",
    "\"\"\"\n",
    "xgb_features = X_train.columns\n",
    "\n",
    "\n",
    "xgb_train = X_train[xgb_features]\n",
    "xgb_val = X_validation[xgb_features]\n",
    "xgb_test = X_test[xgb_features]\n",
    "\n",
    "xgb_model = XGBRegressor(max_depth=8, \n",
    "                         n_estimators=500, \n",
    "                         min_child_weight=1000,  \n",
    "                         colsample_bytree=0.7, \n",
    "                         subsample=0.7, \n",
    "                         eta=0.3, \n",
    "                         seed=0)\n",
    "xgb_model.fit(xgb_train, \n",
    "              Y_train, \n",
    "              eval_metric=\"rmse\", \n",
    "              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n",
    "              verbose=20, \n",
    "              early_stopping_rounds=20)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "xgb_train_pred = xgb_model.predict(xgb_train)\n",
    "xgb_val_pred = xgb_model.predict(xgb_val)\n",
    "xgb_test_pred = xgb_model.predict(xgb_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(xgb_train_pred, Y_train))\n",
    "print('rmse of train : ' , rmse)\n",
    "               \n",
    "rmse = np.sqrt(mean_squared_error(xgb_val_pred, Y_validation))\n",
    "print('rmse of validation : ' , rmse)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dmatrix of xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=2017, num_rounds=500):\n",
    "    param = {}\n",
    "    param['objective'] = 'reg:linear'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['eval_metric'] = 'rmse'\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    xgtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgboost.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgboost.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgboost.DMatrix(test_X)\n",
    "        model = xgboost.train(plst, xgtrain, num_rounds)\n",
    "        \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "dmatrix_model = XGB_regressor(train_X = X_train, train_y = Y_train, test_X = X_validation, test_y = Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only part of features on XGBoost.\n",
    "#xgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n",
    " #               'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n",
    "  #              'shop_item_mean', 'item_trend', 'mean_item_cnt']\n",
    "\n",
    "xgb_features = X_train.columns\n",
    "\n",
    "\n",
    "xgb_train = X_train[xgb_features]\n",
    "xgb_val = X_validation[xgb_features]\n",
    "xgb_test = X_test[xgb_features]\n",
    "\n",
    "dmatrix_train_pred = dmatrix_model.predict(xgboost.DMatrix( xgb_train ), ntree_limit = dmatrix_model.best_ntree_limit)\n",
    "dmatrix_val_pred = dmatrix_model.predict(xgboost.DMatrix( xgb_val ), ntree_limit = dmatrix_model.best_ntree_limit)\n",
    "dmatrix_test_pred = dmatrix_model.predict(xgboost.DMatrix( xgb_test ), ntree_limit = dmatrix_model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(Y_train, lightgbm_train_pred ))\n",
    "print('rmse of train : ' , rmse)\n",
    "               \n",
    "rmse = np.sqrt(mean_squared_error(Y_validation, lightgbm_val_pred))\n",
    "print('rmse of validation : ' , rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'nthread': 10,\n",
    "         'max_depth': 5,\n",
    "#         'max_depth': 9,\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'rmse', # this is abs(a-e)/max(1,a)\n",
    "#         'num_leaves': 39,\n",
    "        'num_leaves': 64,\n",
    "        'learning_rate': 0.2,\n",
    "       'feature_fraction': 0.9,\n",
    "#         'feature_fraction': 0.8108472661400657,\n",
    "#         'bagging_fraction': 0.9837558288375402,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'lambda_l1': 3.097758978478437,\n",
    "        'lambda_l2': 2.9482537987198496,\n",
    "#       'lambda_l1': 0.06,\n",
    "#       'lambda_l2': 0.1,\n",
    "        'verbose': 1,\n",
    "        'min_child_weight': 6.996211413900573,\n",
    "        'min_split_gain': 0.037310344962162616,\n",
    "        }\n",
    "    \n",
    "lgbdataset_train = lgb.Dataset(X_train,Y_train)\n",
    "lgbdataset_valid = lgb.Dataset(X_validation,Y_validation)\n",
    "\n",
    "lgbdataset_model = lgb.train(params, lgbdataset_train, 3000, valid_sets=[lgbdataset_train, lgbdataset_valid],early_stopping_rounds=50, verbose_eval=50)\n",
    "\n",
    "lgbdataset_pred_train = lgbdataset_model.predict(X_train)\n",
    "lgbdataset_pred_val = lgbdataset_model.predict(X_validation)\n",
    "lgbdataset_pred_val = lgbdataset_model.predict(test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(lgbdataset_pred_train, Y_train))\n",
    "print('rmse of train : ' , rmse)\n",
    "               \n",
    "rmse = np.sqrt(mean_squared_error(lgbdataset_pred_val, Y_validation))\n",
    "print('rmse of validation : ' , rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "\n",
    "lasso.fit(train, price_train)\n",
    "\n",
    "y_lasso_pred = lasso.predict(train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_lasso_pred , y_train))\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "ENet.fit(train, price_train)\n",
    "\n",
    "y_ENet_pred = ENet.predict(train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_ENet_pred , y_train))\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "KRR.fit(train, price_train)\n",
    "\n",
    "y_KRR_pred = KRR.predict(train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_KRR_pred , y_train))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost.fit(train, price_train)\n",
    "\n",
    "\n",
    "y_GBoost_pred = GBoost.predict(train)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_GBoost_pred , y_train))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .values must be added to x_train\n",
    "\n",
    "stacked_averaged_models.fit(x_train.values, y_train)\n",
    "\n",
    "y_stacked_averaged_models_pred = stacked_averaged_models.predict(x_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_stacked_averaged_models_pred , y_test))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(test['ID'], columns=['ID'])\n",
    "#prediction_df['sales'] = lightgbm_test_pred\n",
    "#prediction_df['sales'] = rf_test_pred\n",
    "#prediction_df['sales'] = lr_test_pred\n",
    "#prediction_df['sales'] = xgb_test_pred\n",
    "#prediction_df['sales'] = dmatrix_test_pred\n",
    "#prediction_df['sales'] = y_lasso_pred\n",
    "#prediction_df['sales'] = y_ENet_pred\n",
    "#prediction_df['sales'] = y_KRR_pred\n",
    "#prediction_df['sales'] = y_GBoost_pred\n",
    "#prediction_df['sales'] = y_stacked_averaged_models_pred\n",
    "#prediction_df['sales'] = lgbdataset_pred_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction_df.to_csv(r'C:\\Users\\pc\\Desktop\\Data Science Folder\\Retail2\\submission.csv', index=False)\n",
    "prediction_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
