{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import IPython\n",
    "from IPython import display\n",
    "import sklearn\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(r'C:\\Users\\pc\\Desktop\\Data Science Folder\\Titanic Machine Learning From Disaster\\train.csv')\n",
    "data_val  = pd.read_csv(r'C:\\Users\\pc\\Desktop\\Data Science Folder\\Titanic Machine Learning From Disaster\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_raw.copy(deep = True)\n",
    "data_cleaner = [data1, data_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:    \n",
    "    \n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "data1.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived      0\n",
      "Pclass        0\n",
      "Name          0\n",
      "Sex           0\n",
      "Age           0\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Embarked      0\n",
      "FamilySize    0\n",
      "IsAlone       0\n",
      "Title         0\n",
      "FareBin       0\n",
      "AgeBin        0\n",
      "dtype: int64\n",
      "----------\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "FamilySize       0\n",
      "IsAlone          0\n",
      "Title            0\n",
      "FareBin          0\n",
      "AgeBin           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data1.isnull().sum() )\n",
    "print(\"-\"*10)\n",
    "print(data_val.isnull().sum() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new columns to data1 and data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:    \n",
    "    \n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "   \n",
    "    dataset['IsAlone'][dataset['FamilySize'] > 1] = 0\n",
    "\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify 'Title' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******before******\n",
      " Mr              517\n",
      "Miss            182\n",
      "Mrs             125\n",
      "Master           40\n",
      "Dr                7\n",
      "Rev               6\n",
      "Col               2\n",
      "Major             2\n",
      "Mlle              2\n",
      "the Countess      1\n",
      "Don               1\n",
      "Jonkheer          1\n",
      "Ms                1\n",
      "Capt              1\n",
      "Lady              1\n",
      "Mme               1\n",
      "Sir               1\n",
      "Name: Title, dtype: int64 \n",
      "\n",
      "******after*******\n",
      " Mr        517\n",
      "Miss      182\n",
      "Mrs       125\n",
      "Master     40\n",
      "Misc       27\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('******before******\\n' , data1['Title'].value_counts(), '\\n')\n",
    "\n",
    "title_names = data1['Title'].value_counts() < 10 \n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "\n",
    "print('******after*******\\n' , data1['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LabelEncoder().fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( LabelEncoder().fit_transform( data1['Embarked'] )[ range(0,20)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adding new columns to data1 and data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:    \n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists containing column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = ['Survived']\n",
    "\n",
    "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']\n",
    "data1_xy =  Target + data1_x\n",
    "\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "\n",
    "\n",
    "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "\n",
    "data1_dummy = pd.get_dummies( data1[data1_x] )\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#**** for learning ***** \n",
    "print( data1_dummy )\n",
    "print ( type(data1_dummy.columns) )\n",
    "L = list(data1_dummy.columns)\n",
    "print(L)\n",
    "print( data1['Sex_Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** for learning *********\n",
    "print( type(data1[data1_x_calc]), '   ' , type(data1[Target]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veryfying the shape of train1_x & test1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Exploratory Analysis with Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print( (data1[[x, Target[0]]].groupby( x, as_index = False )).mean() )\n",
    "        #print('-'*10, '\\n')\n",
    "\n",
    "\n",
    "print( pd.crosstab(data1['Title'], data1[ 'Survived' ]  ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,12])\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x=data1['Fare'])\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(data1['Age'], showmeans = True, meanline = True)\n",
    "plt.title('Age Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']] , stacked=True, color = ['g', 'r'], label = ['Survived','Dead'])\n",
    "plt.title('Fare Histogram by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Age Histogram by Survival')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Family Size Histogram by Survival')\n",
    "plt.xlabel('Family Size (#)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 12])\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.barplot(x = 'Embarked', y = 'Survived', data=data1)\n",
    "plt.subplot(232)\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', order=[3,2,1], data=data1 );\n",
    "plt.subplot(233)\n",
    "sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1 );\n",
    "\n",
    "\n",
    "plt.subplot(234)\n",
    "sns.pointplot(x = 'FareBin', y = 'Survived',  data=data1 );\n",
    "plt.subplot(235)\n",
    "sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1 );\n",
    "plt.subplot(236)\n",
    "sns.pointplot(x = 'FamilySize', y = 'Survived', data=data1 );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
